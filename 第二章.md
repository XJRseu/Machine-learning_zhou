# 第二章 模型评估与选择

### 2.1 经验误差与过拟合

​	机器学习训练过程分为训练集和测试集，在训练集上出现的误差称为训练误差（training error)，在测试集合上出现的误差成为泛化误差（generalization error)，对于模型的评估和实际应用时，通常来说泛化误差越小越好，因为这表明模型的鲁棒性较好。（通常训练误差较小的模型在测试集上的表现往往不尽如人意）

​	当出现训练误差很小，出现泛化误差很大的情况时，这时候就表明产生了过拟合（overfitting)，相对于的是欠拟合（underfitting)，当然，欠拟合的情况比较少出现，更多的是过拟合情况，如何处理过拟合问题是机器学习面临的关键问题。

​	NP难问题：计算机在多项式时间内无法解决的问题，模型无法用一个多项式来表示。

### 2.2 评估方法

​	评估方法主要评估模型的泛化误差能力，首先需要测试集比较符合实际情况，又不能与训练集过于相似（就像老师考试出原题一样，那么没有评分的意义）。本节主要介绍了三种应用比较广泛的机器学习方法。

#### 2.2.1 留出法

​	留出法的设计思想比较浅显，即将所有的样本分成两部分，一部分作为测试集$$S$$，另一部分作为训练集$$T$$,即$$D = S \cup T, S \cap T = \varnothing $$。

​	这种分类方式有很显而易见的问题，测试集好比日常练习，训练集好比考试，如果日常练习难度和考试难度差距过大，考试成绩就无法反应训练的水平，因此在使用留出法的时候尽量保持数据分布的一致性，避免因为数据划分产生偏差。同样，考试和练习的题目组合多种多样，任何题目都可以作为考试题，任何题目也可以作为训练题，所以留出法的另一个问题是存在多种方式对初始的数据集$$D$$进行分割，不同的划分方式会产生不同的结果，因此多采用若干次随机划分，重复实验取平均值进行评估的方式，如进行100次随机划分，留出法就返回这100次结果的平均。

​	留出法的主要目的是评估用数据集$$D$$训练出来的模型性能，所以训练集$$S$$和测试集$$T$$的样本数量需要平衡，否则会降低评估结果的保真性，一般选择2/3~4/5的样本进行训练，其他样本进行测试。

#### 2.2.2 交叉验证法

​	交叉验证法的思想是将数据集$$D$$划分为k个大小相似的互斥子集，每个子集都是分层采样得到的数据，这样每次使用$$k-1$$个数据集进行测试，余下的子集作为验证集，进行k次训练，返回$$k$$次训练的均值，所以交叉验证法也成为$$k$$折交叉验证法。$$k$$折交叉验证法最常用的取值是10，10折交叉验证法的示意图如下图所示：

![](D:\postgraduate\2020\学习资料\西瓜书阅读笔记\picture\k折交叉验证.png)

​	同样，交叉验证法的数据划分方式多种多样，与留出法类似，交叉验证法通常也需要重复p次，最终的评估结果为这个$$p$$次$$k$$折交叉验证的均值。例如10次10折交叉验证法与100次留出法类似，都是进行了100次训练。

​	特别的，当数据集的样本大小为m个时，令$$k = m$$，这样测试集就只有一个样本，为交叉验证法的特例：留一法（LOO）：这种方法训练的模型和实际用$$D$$训练出来的模型比较相似，但是留一法最大的缺点是在样本数量较大的情况下训练时间较长，开销较大

#### 2.2.3 自助法

​	自助法和其他两种方法不同，留出法和交叉验证法总有一部分样本用于测试，使得实际评估的模型比实际的数据集$$D$$小，会产生一些因为样本规模不同导致的估计偏差。

​	为了解决留出法和交叉验证法的上述两种缺点，自助法(bootstrapping)是一个比较好的解决方案。自助法的核心思想是对有$$m$$个数据的数据集$$D$$有放回的重复采样m次，这样就得到了一个包含m个样本的新的数据集$$D'$$，通过概率论的理论知识可知，一个样本在$$m$$次采样中均不出现的概率为$$(1-\frac{1}{m})^m$$,当数据量m足够大时候，该极限为常用极限之一，为$$\frac{1}{e} \approx  0.368$$, 所以通过自助采样法的得到的新数据集$$D'$$中，约有$$36.8$$%的数据未被采样到，使用这样的测试集得到的测试结果称为"包外估计”.

​	自助法这种采样方法在数据集较小，并且难以划分数据集和测试集的时候比较有用，但是这种方法改变了数据集合的初始分布，引入了新的估计偏差。

#### 2.2.4 调参与最终模型

​	大多数的算法都需要对模型使用的参数进行设定，参数往往影响着模型的性能，合适的算法加上良好的参数设定才能训练出一个良好的模型。

​	在实际设计中，是对某个参数选定一个范围和步长，在选定的参数范围内进行测试，选择最佳参数，但是往往这个最佳参数不是真实的最优参数。但是由于神经网络的模型参数数量庞大，仅仅考虑三个参数，每个参数5种可能性，这时候就有125（$$3^5$$)种可能性，所以参数越多模型调参的难度越大。

​	最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。

### 2.3 性能度量

​	性能度量即衡量模型泛化能力的评价标准，不同的模型使用相同的性能度量有不同的评价结果，所以模型的好坏是相对的。

#### 2.3.1 错误率与精度

​	错误率和精度是分类任务中最常用的两种度量方式，错误率是分类错误的样本数占样本总数的笔记，精度是分类正确的样本数占样本综述的比例，对于离散系统，错误率和正确率的的定义为：
$$
E(f;D) = \frac{1}{m} \sum_{i=1}^{m}\mathbb{I}(f(x_i)\neq y_i)\\
acc(f;D) = \frac{1}{m}\sum_{i=1}^{m}\mathbb{I}(f(x_i) = y_i) = 1 - E(f;D)
$$
​	连续的系统中，错误率和正确率可以描述为：
$$
E(f;D) = \int_{x\sim D}\mathbb{I}(f(x_i)\neq y_i )p(x)dx\\
acc(f;D) = \int_{x\sim D}\mathbb{I}(f(x_i)= y_i )p(x)dx = 1-E(f;D)
$$
​	错误率和精度的度量方式多用于分类任务中，既适用于二分类任务也适用于多分类任务。

#### 2.3.2 查准率，查全率与F1

​	相比于错误率和精度，查准率主要用于度量选出的样本有多少判断正确，有多少判断错误，查全率主要用于度量所有正确的样本有多少被挑选了出来，依据样例的真实性和类别与学习器的预测可以划分为四类，真正例（true positive), 假正例（false positive)，真反例（true negative),假反例（false negative)四种，分别用TP,FP,TN,FN来表示，在github上看过一张图，可以比较清晰的记忆这四个情况：

![查准率](D:\postgraduate\2020\学习资料\西瓜书阅读笔记\picture\查准率.png)

​	所以查准率$$P$$和查全率$$R$$分别定义为：
$$
P = \frac{TP}{TP+FP}\\
R = \frac{TP}{TP+FN}
$$
​	查准率和查全率是一对矛盾的度量，查准率高查全率往往偏低，查全率高，查准率往往偏低。“P-R曲线”是根据学习器的预测结果对样本进行排序，排序越前的是越可能是正确的样本，每次可以计算查准率和查全率，绘制PR图。

![PR曲线](D:\postgraduate\2020\学习资料\西瓜书阅读笔记\picture\PR曲线.png)

​	在使用PR图进行学习器的评估比较的时候，如果一个曲线被另一个曲线完全包住，则后者的性能是优于前者的，如图中A学习器的性能优于C学习器的性能，但是PR曲线产生交叉的时候无法断言学习器性能的优劣，这时候可以通过曲线与坐标轴围成的面积来度量学习器的性能。或者通过平衡点（Break-Event Point BEP）（查全率=查准率时的取值）度量两个学习器的好坏，在平衡点处的值越大学习器性能越好。

​	但是通过平衡点来判断学习器性能过于简化，因此引入了F1度量的概念，F1度量是利用查准率和查全率的调和平均定义的：
$$
\frac{1}{F1} = \frac{1}{2} \cdot (\frac{1}{P}+\frac{1}{R})
$$
​	F1度量中加入权重，构成了F1度量更为一般的形式，加全F1度量，定义公式如下：
$$
\frac{1}{F_{\beta}} = \frac{1}{1+\beta^2}(\frac{1}{P} + \frac{\beta^2}{R})
$$
​	若有多个度量矩阵时候，可以利用多个矩阵计算查准率和查全率，计算平均值就得到了”宏查准率“(macro-P),”宏查全率“(macro-R),以及相应的”宏F1“:
$$
macro-P = \frac{1}{n} \sum_{i=1}^{n}P_i\\
macro-R = \frac{1}{n} \sum_{i=1}^{n}R_i\\
\frac{1}{macro-F_1} = \frac{1}{2} \cdot (\frac{1}{macro-P}+\frac{1}{macro-R})
$$
​	宏的F1基于每个矩阵的度量，如果对矩阵里的每个元素分别进行平均，得到$$\overline{TP}, \overline{FP}, \overline{TN}, \overline{FN}$$再基于平均值计算出"微查准率"(micro-P),"微查全率"(micro-R),和"微F1"：
$$
micro-P = \frac{1}{n} \sum_{i=1}^{n}P_i\\
micro-R = \frac{1}{n} \sum_{i=1}^{n}R_i\\
\frac{1}{micro-F_1} = \frac{1}{2} \cdot (\frac{1}{micro-P}+\frac{1}{micro-R})
$$

#### 2.3.3 ROC与AUC

​	ROC：受试者工作特征(Receiver Operating Characteristic)，ROC曲线与PR曲线类似，与PR曲线有区别的是ROC曲线的横轴是“假正例率”(False Positive Rate, FPR),横轴是"真正例率"(True Positive Rate， TPR)，FPR和TPR的定义是：
$$
TPR = \frac{TP}{TP+FN}\\
FPR = \frac{FP}{TN+FP}
$$
​	书上给出了两个ROC曲线的示例图，一个是理想状态下的ROC曲线，另一个是基于有限样例绘制的ROC曲线，通常情况下都是基于有限样例绘制的。和PR图类似，判断两个学习器的好坏通常是判断两个曲线ROC曲线下的面积(AUC Area Under ROC Curve)，

![ROC](D:\postgraduate\2020\学习资料\西瓜书阅读笔记\picture\ROC.png)

#### 2.3.4 代价敏感错误率与代价曲线

​	所谓的代价敏感错误率用通俗的语言来说就是把正确的预测成错误的代价和把错误的预测成正确的代价，但是在非均等代价下，代价局曲线可以反映学习器的期望总体代价，代价局限是经过规范化的取值(normalization)后进行绘制，横轴为正例概率代价，纵轴为取值为[0，1]的归一化代价。

​	代价曲线的绘制：ROC曲线上的每一点坐标为(TPR, FPR)，可以计算出假反例率(FNR),在代价平面上绘制一条从(0, FPR)到(1,FPR)的现段，线段下的面积即表示该条件下的期望总体代价。将ROC曲线上的所有点转移到代价曲线平面后，围成的面积为在所有条件下学习器的期望总体代价。

![代价曲线](D:\postgraduate\2020\学习资料\西瓜书阅读笔记\picture\代价曲线.png)

### 2.4 比较检验

​	比较检验的主要目的是通过在测试集上学习器A和B的测试数据推断学习器A和学习器B在统计学意义上的泛化性能优劣。由于在统计学意义上对学习器A和学习器B的性能进行比较，所以本节会涉及较多统计学公式和概率论方面的知识。

#### 2.4.1 假设检验

​	本节书上用了二项分布的例子来解释假设检验的置信度的问题，其中的理论主要是涉及到概率论部分假设检验和置信度的内容，这部分的内容需要提前复习下假设检验的内容。

​	在机器学习的测试中，采样测试不止一轮，因此每一次训练都会获得一个错误率$$\epsilon$$，这里把每一次错误采样得到的错误率视为真实错误率$$\epsilon_{0}$$的真实采样，将每次训练视作一次样本采样，依据大数定律，我们可以构建一个新的变量$$\tau_t = \frac{\sqrt{k}(\mu - \epsilon_0)}{\sigma}$$服从自由度为k-1的t分布(这里的主要目的是构造一个常用的假设检验分布，这里2.4.1节主要对一个测试集一种算法进行比较检验方法的说明）

#### 2.4.2 交叉验证$$t$$检验

​	那如果我们有两个算法多个测试集的情况下，我们可以用交叉验证$$t$$检验来进行比比较检验。交叉验证$$t$$检验思想可以类似于假设检验中单变量的东西，利用两个学习器的性能差构造一个符合t分布的概率分布进行判断。

​	当然，概率论和数理统计中最重要的一部分是采样的时候需要独立采样，在样本数量有限的情况下，每个样本测试所得到的错误率不是在同分布情况下得到的，这样会过高得估计假设成立得概率，因此本节后面介绍了一种新的"5*2 交叉验证"来缓解这种过高估计的情况。

#### 2.4.3 McNemar检验 2.4.4 Friedman检验与Nemenyi 后续检验

​	该部分涉及到大量的数理统计知识，后面用到再来更新吧~

### 2.5 偏差与方差

​	本节主要介绍了泛化误差的来源，泛化误差可分解为方差，偏差和噪声，并给出了相应的理论证明，内容看一下即可。



​	

